# ==========================
# CLUSTERING KREDITN√çCH KARET + AUTO EPS + UKL√ÅD√ÅN√ç GRAF≈Æ
# ==========================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from scipy.cluster.hierarchy import linkage, dendrogram
import os

# ==========================
# P≈ò√çPRAVA SLO≈ΩKY PRO GRAFY
# ==========================
output_dir = "plots"
os.makedirs(output_dir, exist_ok=True)

# ==========================
# Naƒçten√≠ dat
# ==========================
df = pd.read_csv("CC GENERAL.csv")

print("Z√°kladn√≠ informace o datech:")
print(df.info())
print(df.head())

# ==========================
# ƒåi≈°tƒõn√≠ a p≈ô√≠prava dat
# ==========================
X = df.drop("CUST_ID", axis=1)
X.fillna(X.mean(), inplace=True)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ==========================
# K-MEANS CLUSTERING
# ==========================
sil_scores = []
K = range(2, 10)

for k in K:
    # explicitnƒõ nastav√≠me n_init pro stabilitu v√Ωsledk≈Ø
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    sil = silhouette_score(X_scaled, labels)
    sil_scores.append(sil)

# üìä Silhouette graf
plt.figure(figsize=(6,4))
plt.plot(K, sil_scores, marker='o')
plt.title("Silhouette metoda pro K-Means")
plt.xlabel("Poƒçet cluster≈Ø K")
plt.ylabel("Silhouette sk√≥re")
plt.savefig(os.path.join(output_dir, "silhouette_kmeans.png"))
plt.show()

best_k = K[sil_scores.index(max(sil_scores))]
print(f" Nejlep≈°√≠ poƒçet cluster≈Ø podle silhouette sk√≥re: {best_k}")

kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
df["KMeans_Cluster"] = kmeans.fit_predict(X_scaled)

# ==========================
# DBSCAN CLUSTERING (automatick√© EPS)
# ==========================
# Robustn√≠ volba min_samples (MinPts): minim√°lnƒõ 3, obvykle 2*dim, ale mus√≠ b√Ωt < n_samples
n_samples, n_features = X_scaled.shape
min_pts = max(3, 2 * n_features)
if min_pts >= n_samples:
    # uprav√≠me tak, aby NearestNeighbors n_neighbors < n_samples
    min_pts = max(2, n_samples - 1)

# pokud je st√°le m√©nƒõ ne≈æ 2 vzorky, DBSCAN nema smysl, ale nech√°me to bƒõ≈æet
neighbors = NearestNeighbors(n_neighbors=min_pts)
neighbors_fit = neighbors.fit(X_scaled)
distances, indices = neighbors_fit.kneighbors(X_scaled)
# k-distances (vzd√°lenost k-teho souseda) pro elbow metodu
k_distances = np.sort(distances[:, min_pts-1])

# Automatick√° detekce EPS - hled√°me prudk√Ω n√°r≈Øst (druh√° derivace)
derivative = np.gradient(k_distances)
second_derivative = np.gradient(derivative)
eps_index = int(np.argmax(second_derivative))
auto_eps = float(k_distances[eps_index])

# Fallback: pokud auto_eps nen√≠ smyslupln√©, pou≈æijeme 90. percentil
if not np.isfinite(auto_eps) or auto_eps <= 0:
    auto_eps = float(np.percentile(k_distances, 90))
    if auto_eps <= 0:
        # koneƒçn√Ω fallback ‚Äî mal√© kladn√© ƒç√≠slo
        auto_eps = 1e-6

plt.figure(figsize=(6,4))
plt.plot(k_distances, label="k-vzd√°lenosti")
plt.axvline(eps_index, color='r', linestyle='--', label=f"Navr≈æen√Ω EPS ‚âà {auto_eps:.4f}")
plt.title("Elbow metoda pro DBSCAN (automatick√° detekce Eps)")
plt.xlabel("Vzorky")
plt.ylabel("Vzd√°lenost k nejbli≈æ≈°√≠mu sousedovi")
plt.legend()
plt.savefig(os.path.join(output_dir, "elbow_dbscan.png"))
plt.show()

print(f"ü§ñ Automaticky zvolen√° hodnota eps ‚âà {auto_eps:.4f} (min_pts={min_pts})")

# Spu≈°tƒõn√≠ DBSCAN
dbscan = DBSCAN(eps=auto_eps, min_samples=min_pts)
df["DBSCAN_Cluster"] = dbscan.fit_predict(X_scaled)

# ==========================
# ANAL√ùZA V√ùSLEDK≈Æ
# ==========================
print("\nüìä Pr≈Ømƒõrn√© hodnoty atribut≈Ø podle KMeans cluster≈Ø:")
# explicitnƒõ vypoƒçteme pr≈Ømƒõry bez CUST_ID
cluster_means_kmeans = df.drop(columns=["CUST_ID"]).groupby("KMeans_Cluster").mean()
print(cluster_means_kmeans)

print("\nüìä Pr≈Ømƒõrn√© hodnoty atribut≈Ø podle DBSCAN cluster≈Ø:")
# DBSCAN m≈Ø≈æe obsahovat label -1 (noise)
cluster_means_dbscan = df.drop(columns=["CUST_ID"]).groupby("DBSCAN_Cluster").mean()
print(cluster_means_dbscan)

# ==========================
# AUTOMATICK√â POJMENOV√ÅN√ç CLUSTER≈Æ (Z-SCORE)
# ==========================
def name_clusters_by_zscore(df_full, cluster_col="KMeans_Cluster", drop_cols=None, n_top=3):
    drop_cols = drop_cols or []
    cluster_like_cols = [c for c in df_full.columns if 'Cluster' in c and c != cluster_col]
    all_drop = list(drop_cols) + cluster_like_cols + [cluster_col]
    # vybereme jen ƒç√≠seln√© atributy a bez cluster/ID sloupc≈Ø
    cols = [c for c in df_full.columns if c not in all_drop]
    numeric_cols = df_full[cols].select_dtypes(include=[np.number]).columns.tolist()
    overall_mean = df_full[numeric_cols].mean()
    overall_std = df_full[numeric_cols].std().replace(0, 1.0)
    cluster_means = df_full.groupby(cluster_col)[numeric_cols].mean()
    names = {}
    for cl in cluster_means.index:
        # noise v DBSCAN m≈Ø≈æe b√Ωt -1 ‚Äî pojmenujeme ho explicitnƒõ pozdƒõji
        z = (cluster_means.loc[cl] - overall_mean) / overall_std
        top = z.nlargest(n_top).index.tolist()
        names[cl] = f"Cluster {cl}: " + " / ".join(top)
    return names

cluster_names = name_clusters_by_zscore(df, cluster_col="KMeans_Cluster", drop_cols=["CUST_ID"])
print("\n Navr≈æen√° jm√©na KMeans cluster≈Ø:")
for i, name in cluster_names.items():
    print(f"  - {name}")

# DBSCAN - nezahrnujeme noise (-1) do pojmenov√°n√≠ bƒõ≈æn√Ωch cluster≈Ø
valid_dbscan = df[df["DBSCAN_Cluster"] != -1]
if not valid_dbscan.empty:
    dbscan_names = name_clusters_by_zscore(valid_dbscan, cluster_col="DBSCAN_Cluster", drop_cols=["CUST_ID"])
else:
    dbscan_names = {}
if -1 in df["DBSCAN_Cluster"].unique():
    dbscan_names[-1] = "Noise / Outliers"

print("\nüß© Navr≈æen√° jm√©na DBSCAN cluster≈Ø:")
for i, name in dbscan_names.items():
    print(f"  - {name}")

# Volitelnƒõ silhouette pro DBSCAN ‚Äî pouze pokud jsou alespo≈à 2 clustery (bez noise)
labels_db = df["DBSCAN_Cluster"].values
valid_labels = [l for l in np.unique(labels_db) if l != -1]
sil_db = None
if len(valid_labels) >= 2:
    try:
        sil_db = silhouette_score(X_scaled[labels_db != -1], labels_db[labels_db != -1])
        print(f"\nüìà Silhouette pro DBSCAN (bez noise): {sil_db:.4f}")
    except Exception as e:
        print("Nelze spoƒç√≠tat silhouette pro DBSCAN:", e)

# ==========================
# VIZUALIZACE (PCA 2D)
# ==========================
pca = PCA(2)
X_pca = pca.fit_transform(X_scaled)

plt.figure(figsize=(8,5))
plt.scatter(X_pca[:,0], X_pca[:,1], c=df["KMeans_Cluster"], cmap='rainbow')
plt.title("K-Means Clustery (PCA 2D)")
plt.savefig(os.path.join(output_dir, "pca_kmeans.png"))
plt.show()

plt.figure(figsize=(8,5))
plt.scatter(X_pca[:,0], X_pca[:,1], c=df["DBSCAN_Cluster"], cmap='rainbow')
plt.title("DBSCAN Clustery (PCA 2D)")
plt.savefig(os.path.join(output_dir, "pca_dbscan.png"))
plt.show()

# ==========================
# BONUS: HIERARCHICK√ù CLUSTERING
# ==========================
Z = linkage(X_scaled, method='ward')
plt.figure(figsize=(10,5))
dendrogram(Z, truncate_mode='level', p=5)
plt.title("Hierarchick√Ω clustering - dendrogram")
plt.savefig(os.path.join(output_dir, "hierarchical_dendrogram.png"))
plt.show()

# ==========================
# ULO≈ΩEN√ç V√ùSLEDK≈Æ
# ==========================
df.to_csv("clustered_creditcards.csv", index=False)
print("\nüíæ V√Ωsledky ulo≈æeny do 'clustered_creditcards.csv'")
print(f"üñºÔ∏è Grafy byly ulo≈æeny do slo≈æky: {os.path.abspath(output_dir)}")
print("\n‚úÖ Hotovo! Clustery byly vytvo≈ôeny, pojmenov√°ny, vizualizov√°ny a ulo≈æen√©.")