# -------------------------------------------------------------
# ğŸ§  Implementace PageRank algoritmu podle Google formulace
# Autor: Libor VÃ½leta
# ZadÃ¡nÃ­: Implementovat PageRank a otestovat na testovacÃ­ch i reÃ¡lnÃ½ch datech
# -------------------------------------------------------------

import argparse
import time
from collections import defaultdict, deque
from urllib.parse import urljoin, urlparse, urlunparse
from urllib import robotparser

import numpy as np
import requests
from bs4 import BeautifulSoup


# -------------------------------------------------------------
# Utility: canonicalizace URL a robots.txt parser cache
# -------------------------------------------------------------

def canonicalize_url(url, remove_query=True):
    """Canonicalizuje URL pro lepÅ¡Ã­ deduplikaci: odstranÃ­ fragment, volitelnÄ› query,
    lower-case scheme+netloc, odstranÃ­ standardnÃ­ porty, odstranÃ­ koncovÃ© '/'."""
    parsed = urlparse(url)
    scheme = parsed.scheme.lower()
    netloc = parsed.netloc.lower()

    # odstranit standardnÃ­ porty
    if netloc.endswith(':80') and scheme == 'http':
        netloc = netloc[:-3]
    if netloc.endswith(':443') and scheme == 'https':
        netloc = netloc[:-4]

    path = parsed.path or '/'
    # odstranÃ­me duplicitnÃ­ lomÃ­tka na zaÄÃ¡tku cesty
    # (nemÄ›nÃ­ vÃ½znam, jen konzistence)
    while '//' in path:
        path = path.replace('//', '/')

    query = '' if remove_query else parsed.query
    fragment = ''

    # upravÃ­me path tak, aby root zÅ¯stal '/'
    path_norm = path.rstrip('/')
    if path_norm == '':
        path_norm = '/'
    canon = urlunparse((scheme, netloc, path_norm, '', query, fragment))
    return canon


_robot_parsers = {}


def get_robot_parser_for(url, session=None):
    """VrÃ¡tÃ­ RobotFileParser pro danou domÃ©nu (cache)."""
    parsed = urlparse(url)
    base = f"{parsed.scheme}://{parsed.netloc}"
    if base in _robot_parsers:
        return _robot_parsers[base]

    rp = robotparser.RobotFileParser()
    robots_url = base + '/robots.txt'
    try:
        # pouÅ¾ijeme requests, aby se respektovala pÅ™esmÄ›rovÃ¡nÃ­ a timeout
        s = session or requests.Session()
        r = s.get(robots_url, timeout=5)
        if r.status_code == 200:
            rp.parse(r.text.splitlines())
        else:
            # pokud robots.txt nenÃ­ dostupnÃ½, povaÅ¾ujeme to za povolenÃ©
            rp = None
    except Exception:
        rp = None

    _robot_parsers[base] = rp
    return rp


# -------------------------------------------------------------
# VylepÅ¡enÃ¡ funkce get_links
# -------------------------------------------------------------

def get_links(url, base_netloc,
              session=None,
              include_subdomains=True,
              allow_query=False,
              exclude_exts=None,
              max_links=0,
              timeout=5,
              user_agent='PageRankBot/1.0'):
    """
    ZÃ­skÃ¡ odkazy ze strÃ¡nky `url`, kterÃ© patÅ™Ã­ do domÃ©ny `base_netloc`.
    VrÃ¡tÃ­ mnoÅ¾inu canonicalizovanÃ½ch URL.

    - include_subdomains: pokud True, povolÃ­ subdomÃ©ny (napÅ™. www., sport.)
    - allow_query: pokud False, odstranÃ­ query string
    - exclude_exts: tuple pÅ™Ã­pon, kterÃ© budou ignorovÃ¡ny
    - max_links: 0 = neomezenÄ›, jinak max poÄet odkazÅ¯ vrÃ¡cenÃ½ch z tÃ©to strÃ¡nky
    - timeout: HTTP timeout
    """
    session = session or requests.Session()
    headers = {'User-Agent': user_agent}

    if exclude_exts is None:
        exclude_exts = ('.pdf', '.jpg', '.jpeg', '.png', '.gif', '.svg', '.zip', '.rar', '.exe', '.mp4', '.mp3')

    # robots
    rp = get_robot_parser_for(url, session=session)
    if rp is not None:
        try:
            if not rp.can_fetch(user_agent, url):
                # nepovolenÃ© podle robots.txt
                # vrÃ¡tÃ­me prÃ¡zdnÃ© mnoÅ¾iny
                return set()
        except Exception:
            # pokud RP selÅ¾e, pokraÄujeme (pÅ™Ã­snost nenÃ­ kritickÃ¡)
            pass

    try:
        resp = session.get(url, timeout=timeout, headers=headers)
        resp.raise_for_status()
    except Exception as e:
        # tiskneme struÄnÄ› chybu a vracÃ­me prÃ¡zdnÃ©
        print(f"  âš ï¸  Chyba pÅ™i stahovÃ¡nÃ­ {url}: {e}")
        return set()

    soup = BeautifulSoup(resp.text, 'html.parser')
    links = set()

    for a in soup.find_all('a', href=True):
        href_raw = a['href'].strip()
        # ignoruj ne-url schÃ©mata
        if href_raw.startswith(('mailto:', 'javascript:', 'tel:', 'sms:')):
            continue

        joined = urljoin(url, href_raw)
        parsed = urlparse(joined)
        if parsed.scheme not in ('http', 'https'):
            continue

        # canonicalizace
        canonical = canonicalize_url(joined, remove_query=not allow_query)
        p = urlparse(canonical)

        # filtrovÃ¡nÃ­ pÅ™Ã­pon
        path = p.path.lower()
        if any(path.endswith(ext) for ext in exclude_exts):
            continue

        # domÃ©na/subdomÃ©na filtr
        if include_subdomains:
            if base_netloc not in p.netloc:
                continue
        else:
            if p.netloc != base_netloc:
                continue

        links.add(canonical)
        if max_links and len(links) >= max_links:
            break

    return links


# -------------------------------------------------------------
# CrawlovacÃ­ funkce: BFS do zadanÃ© hloubky (2 dle zadÃ¡nÃ­)
# -------------------------------------------------------------

def crawl(start_url, depth=2, max_pages=500, max_links_per_page=0,
          include_subdomains=True, allow_query=False, user_agent='PageRankBot/1.0'):
    """
    Provede crawling od start_url do zadanÃ© hloubky (inkl. start_url jako level 0).
    VrÃ¡tÃ­ list tuple (src, dst) bez duplicit.
    - max_pages: maximÃ¡lnÃ­ poÄet navÅ¡tÃ­venÃ½ch strÃ¡nek (ochrana proti runaway)
    - max_links_per_page: 0 = neomezenÄ›
    """
    session = requests.Session()
    start_canon = canonicalize_url(start_url, remove_query=not allow_query)
    base_netloc = urlparse(start_canon).netloc

    visited = set()
    dataset = set()

    queue = deque([(start_canon, 0)])

    while queue and len(visited) < max_pages:
        url, lvl = queue.popleft()
        if url in visited:
            continue
        if lvl > depth:
            continue

        # zkontrolovat robots, get_links takÃ© kontroluje
        links = get_links(url, base_netloc, session=session,
                          include_subdomains=include_subdomains,
                          allow_query=allow_query,
                          exclude_exts=None,
                          max_links=max_links_per_page,
                          timeout=5,
                          user_agent=user_agent)

        visited.add(url)

        for dst in links:
            # vynechat self-linky (url -> url)
            if dst == url:
                continue
            dataset.add((url, dst))
            # pokud jeÅ¡tÄ› nevyuÅ¾ili budget a lvl < depth, pÅ™idej do fronty
            if dst not in visited and lvl + 1 <= depth:
                queue.append((dst, lvl + 1))

        # ohleduplnÃ© zpomalenÃ­
        time.sleep(0.15)

    # vracÃ­me jako list pro kompatibilitu s pagerank
    return list(dataset)


# -------------------------------------------------------------
#  PageRank - pÅ™esnÄ› podle zadÃ¡nÃ­ (matice A explicitnÄ› sloÅ¾enÃ¡)
# -------------------------------------------------------------

def pagerank(links, beta=0.85, iterations=50):
    """
    r(0) = 1/N
    r(t+1) = A . r(t)
    A = beta*M + (1-beta)*(1/N)*E
    """
    pages = sorted(set([src for src, dst in links] + [dst for src, dst in links]))
    N = len(pages)
    if N == 0:
        return {}

    index = {p: i for i, p in enumerate(pages)}

    # Build adjacency from unique outgoing neighbors to avoid duplicates and self-loops
    neighbors = defaultdict(set)
    for src, dst in links:
        if src in index and dst in index and src != dst:
            neighbors[src].add(dst)

    M = np.zeros((N, N))
    # out_degree = number of unique outgoing neighbors
    for src, dsts in neighbors.items():
        j = index[src]
        k = len(dsts)
        if k == 0:
            continue
        prob = 1.0 / k
        for dst in dsts:
            i = index[dst]
            M[i, j] = prob

    # dangling nodes
    for j in range(N):
        if np.sum(M[:, j]) == 0:
            M[:, j] = 1.0 / N

    E = np.ones((N, N))
    A = beta * M + (1 - beta) * (1 / N) * E

    r = np.ones(N) / N
    for _ in range(iterations):
        r = A @ r

    return {pages[i]: float(r[i]) for i in range(N)}


# -------------------------------------------------------------
# TestovacÃ­ data a hlavnÃ­ bÄ›h
# -------------------------------------------------------------

def test_pagerank_print():
    test_links = [
        (1, 2), (1, 3),
        (2, 4),
        (3, 1), (3, 2), (3, 4),
        (4, 3)
    ]
    print('\n' + '=' * 60)
    print('ğŸ”¹ TESTOVACÃ DATA â€“ ukÃ¡zka r(0) â†’ r(50)')
    print('=' * 60)

    pages = sorted(set([src for src, dst in test_links] + [dst for src, dst in test_links]))
    N = len(pages)

    # sestavÃ­me M a A jako v zadÃ¡nÃ­
    index = {p: i for i, p in enumerate(pages)}
    M = np.zeros((N, N))
    out_degree = defaultdict(int)
    for src, dst in test_links:
        out_degree[src] += 1
    for src, dst in test_links:
        M[index[dst], index[src]] = 1.0 / out_degree[src]
    for j in range(N):
        if np.sum(M[:, j]) == 0:
            M[:, j] = 1.0 / N
    beta = 0.85
    E = np.ones((N, N))
    A = beta * M + (1 - beta) * (1 / N) * E

    # r(0)
    r0 = np.ones(N) / N
    print(f'r(0): {np.round(r0, 8)}')

    # provÃ©st 50 iteracÃ­ (r(50)) a ovÄ›Å™it souÄet
    iterations = 50
    r = r0.copy()
    for t in range(1, iterations + 1):
        r = A @ r

    print(f'r({iterations}): {np.round(r, 8)}')

    print(f'\nğŸ“Š VÃ½slednÃ© ranky (po {iterations}. iteraci):')
    for page_num, score in sorted(zip(pages, r), key=lambda x: -x[1]):
        print(f'  StrÃ¡nka {page_num}: {score:.6f}')

    total = float(np.sum(r))
    print(f'\nSuma PageRankÅ¯: {total:.12f}')
    # ovÄ›Å™enÃ­, Å¾e suma je pÅ™ibliÅ¾nÄ› 1 (malÃ© numerickÃ© odchylky povoleny)
    assert abs(total - 1.0) < 1e-9, f'Suma PageRankÅ¯ nenÃ­ 1 (hodnota: {total})'
    print('âœ… Test: suma PageRank hodnot je â‰ˆ 1 (ok)')


def main():
    parser = argparse.ArgumentParser(description='Crawler + PageRank podle zadÃ¡nÃ­')
    parser.add_argument('--start', default='https://ailab.fai.utb.cz/', help='StartovacÃ­ URL (default: ailab)')
    parser.add_argument('--depth', type=int, default=2, help='Hloubka crawl (default 2)')
    parser.add_argument('--max_pages', type=int, default=300, help='Max poÄet navÅ¡tÃ­venÃ½ch strÃ¡nek')
    parser.add_argument('--max_links_per_page', type=int, default=0, help='Max odkazÅ¯ z jednÃ© strÃ¡nky (0 = neomezenÄ›)')
    parser.add_argument('--iterations', type=int, default=50, help='PoÄet iteracÃ­ PageRank (default 50)')
    args = parser.parse_args()

    # 1) testovacÃ­ pÅ™Ã­klad
    test_pagerank_print()

    # 2) crawling reÃ¡lnÃ©ho webu
    print('\n' + '=' * 60)
    print('ğŸŒ STAHOVÃNÃ ODKAZÅ® Z WEBU (vytvoÅ™enÃ­ datasetu)')
    print('=' * 60)
    print(f'Start URL: {args.start}  |  Hloubka: {args.depth}  |  Max pages: {args.max_pages}')

    dataset = crawl(args.start, depth=args.depth, max_pages=args.max_pages,
                    max_links_per_page=args.max_links_per_page,
                    include_subdomains=True, allow_query=False)

    # odstranÄ›nÃ­ duplicit (uÅ¾ mÃ¡me jako set v crawl, ale pÅ™ebÃ­rÃ¡me list)
    dataset = list(set(dataset))

    # UloÅ¾it dataset do CSV pro dalÅ¡Ã­ analÃ½zu
    try:
        import csv
        with open('dataset.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['source', 'target'])
            for s, d in dataset:
                writer.writerow([s, d])
        print("\nğŸ’¾ Dataset uloÅ¾en do: dataset.csv")
    except Exception as e:
        print(f"âš ï¸ Chyba pÅ™i uklÃ¡dÃ¡nÃ­ datasetu: {e}")

    print(f'\nâœ… VSTUPNÃ DATA â€” poÄet odkazÅ¯ (pÃ¡rÅ¯ src->dst): {len(dataset)}')
    print('\nğŸ“‹ UkÃ¡zka prvnÃ­ch 20 zÃ¡znamÅ¯:')
    for i, (s, d) in enumerate(dataset[:20], 1):
        print(f'  {i:3d}. {s} -> {d}')

    # 3) PageRank
    print('\n' + '=' * 60)
    print('ğŸ VÃPOÄŒET PAGERANKU')
    print('=' * 60)

    ranking = pagerank(dataset, beta=0.85, iterations=args.iterations)

    # UloÅ¾it vÃ½sledky PageRank do CSV (vÅ¾dy vytvoÅ™Ã­me soubor s hlaviÄkou, i kdyÅ¾ je prÃ¡zdnÃ½)
    try:
        import csv
        with open('pagerank.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['url', 'pagerank'])
            if ranking:
                sorted_rank = sorted(ranking.items(), key=lambda x: -x[1])
                for url, score in sorted_rank:
                    writer.writerow([url, score])
        print("\nğŸ’¾ PageRank vÃ½sledky uloÅ¾eny do: pagerank.csv")
    except Exception as e:
        print(f"âš ï¸ Chyba pÅ™i uklÃ¡dÃ¡nÃ­ PageRank vÃ½sledkÅ¯: {e}")

    # Pokud nenÃ­ Å¾Ã¡dnÃ½ vÃ½sledek, oÅ¡etÅ™Ã­me vÃ½stup a vyhneme se numpy warnings
    if not ranking:
        print('\nâš ï¸  Nebyly nalezeny Å¾Ã¡dnÃ© strÃ¡nky pro vÃ½poÄet PageRanku. PlnÃ© vÃ½stupy jsou pÅ™eskoÄeny.')
        print('\nğŸ“Š Statistiky:')
        print(f'   CelkovÃ½ poÄet strÃ¡nek: 0')
        print(f'   Suma PageRankÅ¯: 0.000000')
        print(f'   PrÅ¯mÄ›rnÃ½ PageRank: 0.000000')
        return

    sorted_rank = sorted(ranking.items(), key=lambda x: -x[1])

    # VykreslenÃ­ grafu top 20 (pokud matplotlib je dostupnÃ½)
    try:
        import matplotlib
        import matplotlib.pyplot as plt

        top_n = 20
        top = sorted_rank[:top_n]
        if top:
            labels = [u for u, s in top]
            scores = [s for u, s in top]
            plt.figure(figsize=(12, 6))
            bars = plt.bar(range(len(scores)), scores, color='C0')
            plt.xticks(range(len(labels)), labels, rotation=75, ha='right')
            plt.ylabel('PageRank')
            plt.title(f'Top {min(top_n, len(labels))} PageRank')
            plt.tight_layout()
            out_png = 'pagerank_top20.png'
            plt.savefig(out_png, dpi=150)
            plt.close()
            print(f"ğŸ’¾ Graf top {min(top_n, len(labels))} uloÅ¾en do: {out_png}")
    except Exception as e:
        # pokud matplotlib nenÃ­ nainstalovanÃ½, instrukce pro uÅ¾ivatele
        if isinstance(e, ModuleNotFoundError):
            print("\nâš ï¸ modul 'matplotlib' nenÃ­ nainstalovanÃ½. Pro vytvoÅ™enÃ­ grafu nainstalujte ho: pip install matplotlib")
        else:
            print(f"\nâš ï¸ Chyba pÅ™i vytvÃ¡Å™enÃ­ grafu: {e}")

    print('\nğŸ† Top 20 nejdÅ¯leÅ¾itÄ›jÅ¡Ã­ch strÃ¡nek:')
    for i, (url, score) in enumerate(sorted_rank[:20], 1):
        print(f'  {i:2d}. {score:.6f} - {url}')

    print('\nğŸ“Š Statistiky:')
    print(f'   CelkovÃ½ poÄet strÃ¡nek: {len(ranking)}')
    print(f'   Suma PageRankÅ¯: {sum(ranking.values()):.6f}')
    print(f'   PrÅ¯mÄ›rnÃ½ PageRank: {np.mean(list(ranking.values())):.6f}')


if __name__ == '__main__':
    main()
